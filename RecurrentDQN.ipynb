{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0db4cc6e",
   "metadata": {},
   "source": [
    "In this file, we write necessary policy components to implement a custom Recurrent Replay model, which implements frameworks from the following papers: \n",
    "\n",
    "@misc{hausknecht2017deeprecurrentqlearningpartially,\n",
    "      title={Deep Recurrent Q-Learning for Partially Observable MDPs}, \n",
    "      author={Matthew Hausknecht and Peter Stone},\n",
    "      year={2017},\n",
    "      eprint={1507.06527},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.LG},\n",
    "      url={https://arxiv.org/abs/1507.06527}, \n",
    "}\n",
    "\n",
    "@inproceedings{\n",
    "      kapturowski2018recurrent,\n",
    "      title={Recurrent Experience Replay in Distributed Reinforcement Learning},\n",
    "      author={Steven Kapturowski and Georg Ostrovski and Will Dabney and John Quan and Remi Munos},\n",
    "      booktitle={International Conference on Learning Representations},\n",
    "      year={2019},\n",
    "      url={https://openreview.net/forum?id=r1lyTjAqYX},\n",
    "}\n",
    "\n",
    "It is important to note that we are creating R2D1 (D1 since we don't need distributed framework) which requires the following improvements on the standard DQN: \n",
    "1) Double DQN\n",
    "2) Prioritized Experience Replay\n",
    "3) Dueling Network\n",
    "4) n-step Learning \n",
    "\n",
    "Additionally we credit: https://github.com/Curt-Park/rainbow-is-all-you-need for base implementation of DQN (with some rainbow elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2337edf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing necessary libraries\n",
    "import os\n",
    "import random\n",
    "from collections import deque\n",
    "from typing import Deque, Dict, List, Tuple, Any\n",
    "import time\n",
    "import datetime\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from segment_tree import MinSegmentTree, SumSegmentTree\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a244287a",
   "metadata": {},
   "source": [
    "Below we implement a DictReplayBuffer, that is adjusted for N-step learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1c9d2263",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictReplayBufferNStep: \n",
    "\n",
    "    \"\"\"\n",
    "    A replay buffer that stores transitions (nested dictionary of NumPy arrays, since obs are returned as a dictionary) and supports N-step returns.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            observation_space: spaces.Dict,\n",
    "            buffer_size: int, \n",
    "            batch_size: int = 64, \n",
    "            n_step: int = 1, \n",
    "            gamma: float = 0.99,\n",
    "    ): \n",
    "        \n",
    "        # adding (o,a,o',r,d) to the buffer\n",
    "        self.observations_buffer = {k: np.zeros([buffer_size, *v.shape], dtype=v.dtype) for k, v in observation_space.spaces.items()}                # observations\n",
    "        self.next_observations_buffer = {k: np.zeros([buffer_size, *v.shape], dtype=v.dtype) for k, v in observation_space.spaces.items()}           # next observations\n",
    "        self.actions_buffer = np.zeros([buffer_size], dtype=np.int64)                                                                                # actions\n",
    "        self.rewards_buffer = np.zeros([buffer_size], dtype=np.float32)                                                                              # rewards\n",
    "        self.dones_buffer = np.zeros([buffer_size], dtype=np.bool_)                                                                                  # dones  \n",
    "\n",
    "        self.ptr, self.size, = 0, 0\n",
    "        self.max_size, self.batch_size = buffer_size, batch_size\n",
    "\n",
    "        # for N-step Learning\n",
    "        self.n_step_buffer = deque(maxlen=n_step)\n",
    "        self.n_step = n_step\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def store(\n",
    "      self, \n",
    "      obs: Dict[str, np.ndarray], \n",
    "      act: float, \n",
    "      rew: float, \n",
    "      next_obs: Dict[str, np.ndarray], \n",
    "      done: bool\n",
    "    ):      \n",
    "        \"\"\"\n",
    "        Store a new transition in the replay buffer, using N-step returns.\n",
    "        \"\"\"\n",
    "\n",
    "        transition = (obs, act, rew, next_obs, done)\n",
    "        self.n_step_buffer.append(transition)\n",
    "\n",
    "         # single step transition is not ready\n",
    "        if len(self.n_step_buffer) < self.n_step:\n",
    "            return ()\n",
    "        \n",
    "        rew, next_obs, done = self._get_n_step_info(\n",
    "            self.n_step_buffer, self.gamma\n",
    "        )\n",
    "        obs, act = self.n_step_buffer[0][:2]\n",
    "\n",
    "        # store all obs keys in buffer \n",
    "        for k in obs:\n",
    "            self.observations_buffer[k][self.ptr] = obs[k]\n",
    "            self.next_observations_buffer[k][self.ptr] = next_obs[k]\n",
    "\n",
    "        self.actions_buffer[self.ptr] = act\n",
    "        self.rewards_buffer[self.ptr] = rew\n",
    "        self.dones_buffer[self.ptr] = done\n",
    "\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "        return self.n_step_buffer[0]  \n",
    "    \n",
    "    def sample_batch(\n",
    "           self) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "        \n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        \n",
    "        return {\n",
    "            'obs': {k: v[idxs] for k, v in self.observations_buffer.items()},\n",
    "            'next_obs': {k: v[idxs] for k, v in self.next_observations_buffer.items()},\n",
    "            'acts': self.actions_buffer[idxs],\n",
    "            'rews': self.rewards_buffer[idxs],\n",
    "            'done': self.dones_buffer[idxs],\n",
    "            # for N-step Learning\n",
    "            'indices': idxs,\n",
    "        }\n",
    "\n",
    "    def sample_batch_from_idxs(self, idxs: np.ndarray) -> Dict[str, Dict[str, np.ndarray]]:\n",
    "        \n",
    "        # for N-step Learning\n",
    "        return {\n",
    "            'obs': {k: v[idxs] for k, v in self.observations_buffer.items()},\n",
    "            'next_obs': {k: v[idxs] for k, v in self.next_observations_buffer.items()},\n",
    "            'acts': self.actions_buffer[idxs],\n",
    "            'rews': self.rewards_buffer[idxs],\n",
    "            'done': self.dones_buffer[idxs],\n",
    "            }\n",
    "    \n",
    "    def _get_n_step_info(\n",
    "            self,\n",
    "            n_step_buffer: Deque,\n",
    "            gamma: float,\n",
    "    ) -> Tuple[float, Dict[str, np.ndarray], bool]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Compute the N-step return for a given buffer of transitions.\n",
    "        \"\"\"\n",
    "\n",
    "         # info of the last transition\n",
    "        rew, next_obs, done = n_step_buffer[-1][-3:]\n",
    "        for transition in reversed(list(n_step_buffer)[:-1]):\n",
    "            r, n_o, d = transition[-3:]\n",
    "            rew = r + gamma * rew * (1 - d)\n",
    "            next_obs, done = (n_o, d) if d else (next_obs, done)\n",
    "        return rew, next_obs, done\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return self.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d7787",
   "metadata": {},
   "source": [
    "Below we create a Prioritized Replay Buffer which is used to implement PER. Note that we inherit from the parent DictReplayBufferNStep class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5159d9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedDictReplayBuffer(DictReplayBufferNStep):\n",
    "\n",
    "    \"\"\"\n",
    "    A prioritized replay buffer that stores transitions (with the observation being a dictionary) and supports N-step returns.\n",
    "\n",
    "    Attributes:\n",
    "        max_priority (float): max priority\n",
    "        tree_ptr (int): next index of tree\n",
    "        alpha (float): alpha parameter for prioritized replay buffer\n",
    "        sum_tree (SumSegmentTree): sum tree for prior\n",
    "        min_tree (MinSegmentTree): min tree for min prior to get max weight\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self, \n",
    "            observation_space: spaces.Dict,\n",
    "            buffer_size: int, \n",
    "            batch_size: int = 64, \n",
    "            alpha: float = 0.6,\n",
    "            n_step: int = 1, \n",
    "            gamma: float = 0.99,\n",
    "    ):\n",
    "        \n",
    "        assert alpha >= 0\n",
    "\n",
    "        super().__init__(observation_space, buffer_size, batch_size, n_step, gamma)\n",
    "        self.max_priority, self.tree_ptr = 1.0, 0\n",
    "        self.alpha = alpha\n",
    "\n",
    "        # capacity must be positive and a power of 2.\n",
    "        tree_capacity = 1\n",
    "        while tree_capacity < self.max_size:\n",
    "            tree_capacity *= 2\n",
    "\n",
    "        self.sum_tree = SumSegmentTree(tree_capacity)\n",
    "        self.min_tree = MinSegmentTree(tree_capacity)\n",
    "\n",
    "    def store(\n",
    "      self, \n",
    "      obs: Dict[str, np.ndarray], \n",
    "      act: float, \n",
    "      rew: float, \n",
    "      next_obs: Dict[str, np.ndarray], \n",
    "      done: bool\n",
    "    ):      \n",
    "        \n",
    "        \"\"\"Store experience and its priority.\"\"\"\n",
    "\n",
    "        transition = super().store(obs, act, rew, next_obs, done)\n",
    "\n",
    "        if transition:\n",
    "            self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "            self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha\n",
    "            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n",
    "        \n",
    "        return transition\n",
    "\n",
    "    def sample_batch(\n",
    "            self, \n",
    "            beta: float = 0.4, \n",
    "        ) -> Dict[str, np.ndarray]: \n",
    "\n",
    "        assert len(self) >= self.batch_size\n",
    "        assert beta > 0\n",
    "\n",
    "        indices = self._sample_proportional()\n",
    "\n",
    "        return {\n",
    "            'obs': {k: self.observations_buffer[k][indices] for k in self.observations_buffer.keys()},\n",
    "            'next_obs': {k: self.next_observations_buffer[k][indices] for k in self.next_observations_buffer.keys()},\n",
    "            'acts': self.actions_buffer[indices],\n",
    "            'rews': self.rewards_buffer[indices],\n",
    "            'done': self.dones_buffer[indices],\n",
    "            # for N-step Learning and PER\n",
    "            'indices': indices,\n",
    "            'weights': np.array([self._calculate_weight(i, beta) for i in indices], dtype=np.float32)\n",
    "\n",
    "        }\n",
    "    \n",
    "    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n",
    "        \"\"\"Update priorities of sampled transitions.\"\"\"\n",
    "        assert len(indices) == len(priorities)\n",
    "\n",
    "        for idx, priority in zip(indices, priorities):\n",
    "            assert priority > 0\n",
    "            assert 0 <= idx < len(self)\n",
    "\n",
    "            self.sum_tree[idx] = priority ** self.alpha\n",
    "            self.min_tree[idx] = priority ** self.alpha\n",
    "\n",
    "            self.max_priority = max(self.max_priority, priority)\n",
    "            \n",
    "    def _sample_proportional(self) -> List[int]:\n",
    "        \"\"\"Sample indices based on proportions.\"\"\"\n",
    "        indices = []\n",
    "        p_total = self.sum_tree.sum(0, len(self) - 1)\n",
    "        segment = p_total / self.batch_size\n",
    "        \n",
    "        for i in range(self.batch_size):\n",
    "            a = segment * i\n",
    "            b = segment * (i + 1)\n",
    "            upperbound = random.uniform(a, b)\n",
    "            idx = self.sum_tree.retrieve(upperbound)\n",
    "            indices.append(idx)\n",
    "            \n",
    "        return indices\n",
    "    \n",
    "    def _calculate_weight(self, idx: int, beta: float):\n",
    "        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n",
    "        # get max weight\n",
    "        p_min = self.min_tree.min() / self.sum_tree.sum()\n",
    "        max_weight = (p_min * len(self)) ** (-beta)\n",
    "        \n",
    "        # calculate weights\n",
    "        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n",
    "        weight = (p_sample * len(self)) ** (-beta)\n",
    "        weight = weight / max_weight\n",
    "        \n",
    "        return weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1f3bda",
   "metadata": {},
   "source": [
    "We implement a network class that uses our custom feature extractor and creates dueling heads. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a388e435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Implements the DQN network architecture that integrates:\n",
    "- Custom feature extraction (via CustomFeatureExtractor for dict observations)\n",
    "- Dueling network architecture\n",
    "\n",
    "The CustomFeatureExtractor processes our specific observation space before feeding them into the noisy dueling head.\n",
    "\"\"\"\n",
    "\n",
    "from CustomFeatureExtractor import CustomFeatureExtractor\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_extractor: nn.Module, \n",
    "        out_dim: int, \n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            feature_extractor (nn.Module): Predefined feature extractor module\n",
    "                (which for us is the CustomFeatureExtractor) that converts dict observations into a flat feature tensor. Different keys will get passed\n",
    "                through different layers as defined in the CustomFeatureExtractor.\n",
    "            out_dim (int): Number of discrete actions.\n",
    "        \"\"\"\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "        \n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "        in_dim = feature_extractor._features_dim # output dim of feature extractor \n",
    "\n",
    "        # set advantage layer\n",
    "        self.advantage_layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, out_dim),\n",
    "        )\n",
    "\n",
    "        # set value layer\n",
    "        self.value_layer = nn.Sequential(\n",
    "            nn.Linear(in_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, obs: dict) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "\n",
    "        features = self.feature_extractor(obs)\n",
    "        value = self.value_layer(features)\n",
    "        advantage = self.advantage_layer(features)  \n",
    "        q = value + advantage - advantage.mean(dim=1, keepdim=True)\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87bb88b",
   "metadata": {},
   "source": [
    "We now implement the Recurrent Replay DQN agent class with the following methods: \n",
    "    1) select_action:\tselect an action from the input state.\n",
    "    2) step:\ttake an action and return the response of the env.\n",
    "    3) compute_dqn_loss:\treturn dqn loss.\n",
    "    4) update_model:\tupdate the model by gradient descent.\n",
    "    5) target_hard_update:\thard update from the local model to the target model.\n",
    "    6) train:\ttrain the agent for an amount of timesteps\n",
    "    8) plot: plot the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcaa6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    \"\"\"\n",
    "    DQN Agent interacting with environment.\n",
    "\n",
    "    Modified to:\n",
    "    1. Handle dictionary observations.\n",
    "    2. Log training data to TensorBoard (SB3-style keys).\n",
    "    3. Save the final model as a .zip file (SB3-style).\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gymnasium.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        seed: int,\n",
    "        gamma: float = 0.99,\n",
    "\n",
    "        # standard DQN epsilon greedy parameters\n",
    "        epsilon_decay: float = 0.05,\n",
    "        max_epsilon: float = 1.0,\n",
    "        min_epsilon: float = 0.1,\n",
    "\n",
    "        # PER parameters\n",
    "        alpha: float = 0.2,\n",
    "        beta: float = 0.6,\n",
    "        prior_eps: float = 1e-6,\n",
    "\n",
    "        # N-step Learning\n",
    "        n_step: int = 3,\n",
    "        lr: float = 2.5e-4,\n",
    "\n",
    "        # TensorBoard run name (subfolder inside models/DQN_tensorboard)\n",
    "        tb_run_name: str = None,\n",
    "    ):\n",
    "        \n",
    "        \"\"\"\n",
    "        initialization of DQN Agent:\n",
    "        \"\"\"\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        self.env = env\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update\n",
    "        self.seed = seed\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = max_epsilon\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        \n",
    "        # PER parameters\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.prior_eps = prior_eps\n",
    "\n",
    "        # N-step parameters\n",
    "        self.n_step = n_step\n",
    "        \n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        \"_______________________________ N-step Learning components _______________________________\"\n",
    "\n",
    "        # memory for 1-step learning \n",
    "        self.memory = PrioritizedDictReplayBuffer(\n",
    "            observation_space=self.env.observation_space,\n",
    "            buffer_size=memory_size,\n",
    "            batch_size=batch_size,\n",
    "            alpha=alpha,\n",
    "            gamma=gamma,   \n",
    "        )\n",
    "        \n",
    "        # memory for N-step Learning\n",
    "        self.use_n_step = True if n_step > 1 else False\n",
    "        if self.use_n_step:\n",
    "            self.n_step = n_step\n",
    "            self.memory_n = DictReplayBufferNStep(\n",
    "                observation_space=self.env.observation_space,\n",
    "                buffer_size=memory_size,\n",
    "                batch_size=batch_size,\n",
    "                n_step=n_step,\n",
    "                gamma=gamma\n",
    "            )\n",
    "        \"______________________________________________________________________________________________\"\n",
    "\n",
    "\n",
    "\n",
    "        \"___________________________ standard DQN and Double DQN components ____________________________\"\n",
    "\n",
    "        # networks: dqn\n",
    "        self.dqn = Network(\n",
    "            CustomFeatureExtractor(self.env.observation_space), \n",
    "            action_dim, \n",
    "        ).to(self.device)\n",
    "\n",
    "        # network: dqn_target\n",
    "        self.dqn_target = Network(\n",
    "            CustomFeatureExtractor(self.env.observation_space), \n",
    "            action_dim, \n",
    "        ).to(self.device)\n",
    "\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters(), lr)\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "        \n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "        # TensorBoard writer\n",
    "        tb_root = \"models/DQN_tensorboard\"\n",
    "        if tb_run_name:\n",
    "            tb_dir = os.path.join(tb_root, tb_run_name)\n",
    "        else:\n",
    "            ts = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "            tb_dir = os.path.join(tb_root, f\"run_{ts}\")\n",
    "        os.makedirs(tb_dir, exist_ok=True)\n",
    "        self.writer = SummaryWriter(log_dir=tb_dir)\n",
    "        self._tb_dir = tb_dir\n",
    "\n",
    "        \"_____________________________________________________________________________________________\"\n",
    "\n",
    "    def _dict_obs_to_tensor(self, obs: Dict[str, np.ndarray]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Helper: Convert a single observation dict to a batched tensor dict.\"\"\"\n",
    "        tensor_obs = {}\n",
    "        for key, value in obs.items():\n",
    "            tensor_obs[key] = torch.as_tensor(value).unsqueeze(0).to(self.device)\n",
    "        return tensor_obs\n",
    "    \n",
    "    def _dict_batch_to_tensor(self, obs_batch: Dict[str, np.ndarray]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Helper: Convert a batch observation dict to a tensor dict.\"\"\"\n",
    "        tensor_obs = {}\n",
    "        for key, value in obs_batch.items():\n",
    "            tensor_obs[key] = torch.as_tensor(value).to(self.device)\n",
    "        return tensor_obs\n",
    "\n",
    "    def select_action(self, obs: np.ndarray) -> np.ndarray:\n",
    "\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        obs_tensor = self._dict_obs_to_tensor(obs)\n",
    "\n",
    "        if self.epsilon > np.random.random():\n",
    "            selected_action = self.env.action_space.sample()\n",
    "        else:\n",
    "            selected_action = self.dqn(obs_tensor).argmax()\n",
    "            selected_action = selected_action.detach().cpu().numpy()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition = [obs_tensor, selected_action]\n",
    "        \n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            \n",
    "            # N-step transition\n",
    "            if self.use_n_step:\n",
    "                one_step_transition = self.memory_n.store(*self.transition)\n",
    "            # 1-step transition\n",
    "            else:\n",
    "                one_step_transition = self.transition\n",
    "\n",
    "            # add a single step transition\n",
    "            if one_step_transition:\n",
    "                self.memory.store(*one_step_transition)\n",
    "    \n",
    "        return next_state, reward, done\n",
    "\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "\n",
    "        # PER needs beta to calculate weights\n",
    "        samples = self.memory.sample_batch(self.beta)\n",
    "        weights = torch.FloatTensor(\n",
    "            samples[\"weights\"].reshape(-1, 1)\n",
    "        ).to(self.device)\n",
    "        indices = samples[\"indices\"]\n",
    "        \n",
    "        # 1-step Learning loss\n",
    "        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n",
    "        \n",
    "        # PER: importance sampling before average\n",
    "        loss = torch.mean(elementwise_loss * weights)\n",
    "        \n",
    "        # N-step Learning loss\n",
    "        # we are gonna combine 1-step loss and n-step loss so as to\n",
    "        # prevent high-variance. The original rainbow employs n-step loss only.\n",
    "        if self.use_n_step:\n",
    "            gamma = self.gamma ** self.n_step\n",
    "            samples = self.memory_n.sample_batch_from_idxs(indices)\n",
    "            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n",
    "            elementwise_loss += elementwise_loss_n_loss\n",
    "            \n",
    "            # PER: importance sampling before average\n",
    "            loss = torch.mean(elementwise_loss * weights)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(self.dqn.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # PER: update priorities\n",
    "        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n",
    "        new_priorities = loss_for_prior + self.prior_eps\n",
    "        self.memory.update_priorities(indices, new_priorities)\n",
    "        \n",
    "        return loss.item()\n",
    "        \n",
    "    def train(self, timesteps: int, plotting_interval: int = 100):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "        \n",
    "        state, _ = self.env.reset(seed=self.seed)\n",
    "        update_cnt = 0\n",
    "        epsilons = []\n",
    "        losses = []\n",
    "        cum_reward = []\n",
    "        ep_lengths = []\n",
    "        ep_length = 0\n",
    "        score = 0\n",
    "\n",
    "        # training timing / fps counters\n",
    "        self._train_start_time = time.time()\n",
    "        self._train_steps = 0\n",
    "\n",
    "        for step in range(1, timesteps + 1):\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done = self.step(action)\n",
    "\n",
    "            # increment step counters\n",
    "            self._train_steps += 1\n",
    "            ep_length += 1\n",
    "\n",
    "            state = next_state\n",
    "            score += reward\n",
    "\n",
    "            # PER: increase beta\n",
    "            fraction = min(step / timesteps, 1.0)\n",
    "            self.beta = self.beta + fraction * (1.0 - self.beta)\n",
    "            \n",
    "            if done:\n",
    "                state, _ = self.env.reset(seed=self.seed)\n",
    "                cum_reward.append(score)\n",
    "                ep_lengths.append(ep_length)\n",
    "                ep_length = 0\n",
    "                score = 0\n",
    "\n",
    "\n",
    "            # if training is ready\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "                \n",
    "                # linearly decrease epsilon\n",
    "                self.epsilon = max(\n",
    "                    self.min_epsilon, self.epsilon - (\n",
    "                        self.max_epsilon - self.min_epsilon\n",
    "                    ) * self.epsilon_decay\n",
    "                )\n",
    "                epsilons.append(self.epsilon)\n",
    "\n",
    "                 # if hard update is needed\n",
    "                if update_cnt % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "\n",
    "            # plotting / TB logging\n",
    "            if step % plotting_interval == 0:\n",
    "                self._plot(cum_reward, losses, step, ep_lengths=ep_lengths)\n",
    "                \n",
    "        self.env.close()\n",
    "        self.writer.close()\n",
    "                \n",
    "    def _compute_dqn_loss(self, samples: Dict[str, Any], gamma: float) -> torch.Tensor:\n",
    "\n",
    "        \"\"\"Return categorical dqn loss, adapted for Dict samples.\"\"\"\n",
    "        device = self.device\n",
    "        \n",
    "        # Convert dictionary observations to tensors\n",
    "        state = self._dict_batch_to_tensor(samples[\"obs\"])\n",
    "        next_state = self._dict_batch_to_tensor(samples[\"next_obs\"])\n",
    "        \n",
    "        # Convert other samples to tensors\n",
    "        action = torch.LongTensor(samples[\"acts\"].reshape(-1,1)).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "        \n",
    "        curr_q_value = self.dqn(state).gather(1, action)\n",
    "        next_q_value = self.dqn_target(next_state).max(dim=1, keepdim=True)[0].detach()\n",
    "\n",
    "        mask = 1 - done\n",
    "        target = (reward + gamma * next_q_value * mask).to(self.device)\n",
    "\n",
    "        # calculate element-wise dqn loss\n",
    "        elementwise_loss = F.smooth_l1_loss(curr_q_value, target, reduction=\"none\")\n",
    "\n",
    "        return elementwise_loss\n",
    "\n",
    "    def _target_hard_update(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "                \n",
    "    def _plot(\n",
    "        self,  \n",
    "        scores: List[float], \n",
    "        losses: List[float],\n",
    "        step: int,\n",
    "        window_size: int = 100, # Window for rolling mean\n",
    "        ep_lengths: List[int] = None,\n",
    "    ):\n",
    "        \"\"\"Log training metrics to TensorBoard using SB3-style keys.\n",
    "\n",
    "        Logged scalars:\n",
    "        - rollout/ep_len_mean\n",
    "        - rollout/ep_rew_mean\n",
    "        - rollout/exploration_rate\n",
    "        - time/fps\n",
    "        - train/learning_rate\n",
    "        - train/loss\n",
    "        \"\"\"\n",
    "\n",
    "        # safe defaults\n",
    "        if ep_lengths is None:\n",
    "            ep_lengths = []\n",
    "\n",
    "        # Mean episode reward\n",
    "        if len(scores) > 0:\n",
    "            mean_reward = np.mean(scores[-window_size:]) if len(scores) >= window_size else np.mean(scores)\n",
    "        else:\n",
    "            mean_reward = 0.0\n",
    "        self.writer.add_scalar(\"rollout/ep_rew_mean\", float(mean_reward), step)\n",
    "\n",
    "        # Rolling mean reward (log last rolling value for visibility)\n",
    "        if len(scores) > window_size:\n",
    "            rolling_mean = np.convolve(scores, np.ones(window_size)/window_size, mode='valid')\n",
    "            self.writer.add_scalar(\"rollout/ep_rew_mean_rolling\", float(rolling_mean[-1]), step)\n",
    "\n",
    "        # Mean episode length\n",
    "        if len(ep_lengths) > 0:\n",
    "            mean_ep_len = np.mean(ep_lengths[-window_size:]) if len(ep_lengths) >= window_size else np.mean(ep_lengths)\n",
    "        else:\n",
    "            mean_ep_len = 0.0\n",
    "        self.writer.add_scalar(\"rollout/ep_len_mean\", float(mean_ep_len), step)\n",
    "\n",
    "        # Exploration rate\n",
    "        self.writer.add_scalar(\"rollout/exploration_rate\", float(self.epsilon), step)\n",
    "\n",
    "        # FPS (frames per second) since training started\n",
    "        elapsed = max(1e-6, time.time() - getattr(self, \"_train_start_time\", time.time()))\n",
    "        fps = float(getattr(self, \"_train_steps\", 0)) / elapsed\n",
    "        self.writer.add_scalar(\"time/fps\", fps, step)\n",
    "\n",
    "        # Training metrics: learning rate and loss\n",
    "        try:\n",
    "            current_lr = float(self.optimizer.param_groups[0][\"lr\"])\n",
    "        except Exception:\n",
    "            current_lr = 0.0\n",
    "        self.writer.add_scalar(\"train/learning_rate\", current_lr, step)\n",
    "\n",
    "        if len(losses) > 0:\n",
    "            mean_loss = np.mean(losses[-window_size:]) if len(losses) >= window_size else np.mean(losses)\n",
    "        else:\n",
    "            mean_loss = 0.0\n",
    "        self.writer.add_scalar(\"train/loss\", float(mean_loss), step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f484a5f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from Env import VoltageDistributionEnv\n",
    "\n",
    "env = VoltageDistributionEnv()\n",
    "agent = DQNAgent(\n",
    "    env= env, \n",
    "    memory_size= 10_000,\n",
    "    batch_size= 64,\n",
    "    target_update= 200,\n",
    "    seed= 42,\n",
    "    gamma= 0.99,\n",
    "\n",
    "    # standard DQN epsilon greedy parameters\n",
    "    epsilon_decay= 0.0001,\n",
    "    max_epsilon= 1.0,\n",
    "    min_epsilon= 0.05,\n",
    "\n",
    "    # PER parameters\n",
    "    alpha= 0.6,\n",
    "    beta= 0.4,\n",
    "    prior_eps= 1e-6,\n",
    "\n",
    "    # N-step Learning\n",
    "    n_step= 1,\n",
    "    lr= 2.5e-4,\n",
    "\n",
    "    # TensorBoard run name (subfolder inside models/DQN_tensorboard)\n",
    "     tb_run_name=\"DQN_New_Trial1\"\n",
    ")\n",
    "\n",
    "agent.train(timesteps=200_000, plotting_interval=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RRDQNvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
